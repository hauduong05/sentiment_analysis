{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hauduong05/sentiment_analysis/blob/main/sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRsJKH486jGO",
        "outputId": "bdcb2e68-e8f1-47c4-a6a5-00e4c1f55e4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlyyOVcGfuzR"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "id": "f_1BX2UPavKj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "5gCPR5x11eBO"
      },
      "outputs": [],
      "source": [
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 687
        },
        "id": "X-1UisycaTHT",
        "outputId": "7d01c78b-bc46-48af-ec45-1bfcce94d607"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           userName                                          userImage  \\\n",
              "0     Andrew Thomas  https://lh3.googleusercontent.com/a-/AOh14GiHd...   \n",
              "1      Craig Haines  https://lh3.googleusercontent.com/-hoe0kwSJgPQ...   \n",
              "2     steven adkins  https://lh3.googleusercontent.com/a-/AOh14GiXw...   \n",
              "3  Lars Panzerbjørn  https://lh3.googleusercontent.com/a-/AOh14Gg-h...   \n",
              "4     Scott Prewitt  https://lh3.googleusercontent.com/-K-X1-YsVd6U...   \n",
              "\n",
              "                                             content  score  thumbsUpCount  \\\n",
              "0  Update: After getting a response from the deve...      1             21   \n",
              "1  Used it for a fair amount of time without any ...      1             11   \n",
              "2  Your app sucks now!!!!! Used to be good but no...      1             17   \n",
              "3  It seems OK, but very basic. Recurring tasks n...      1            192   \n",
              "4  Absolutely worthless. This app runs a prohibit...      1             42   \n",
              "\n",
              "  reviewCreatedVersion                   at  \\\n",
              "0             4.17.0.3  2020-04-05 22:25:57   \n",
              "1             4.17.0.3  2020-04-04 13:40:01   \n",
              "2             4.17.0.3  2020-04-01 16:18:13   \n",
              "3             4.17.0.2  2020-03-12 08:17:34   \n",
              "4             4.17.0.2  2020-03-14 17:41:01   \n",
              "\n",
              "                                        replyContent            repliedAt  \\\n",
              "0  According to our TOS, and the term you have ag...  2020-04-05 15:10:24   \n",
              "1  It sounds like you logged in with a different ...  2020-04-05 15:11:35   \n",
              "2  This sounds odd! We are not aware of any issue...  2020-04-02 16:05:56   \n",
              "3  We do offer this option as part of the Advance...  2020-03-15 06:20:13   \n",
              "4  We're sorry you feel this way! 90% of the app ...  2020-03-15 23:45:51   \n",
              "\n",
              "       sortOrder      appId  \n",
              "0  most_relevant  com.anydo  \n",
              "1  most_relevant  com.anydo  \n",
              "2  most_relevant  com.anydo  \n",
              "3  most_relevant  com.anydo  \n",
              "4  most_relevant  com.anydo  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6b97c99e-a42f-49fb-aca3-e2cd599508af\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userName</th>\n",
              "      <th>userImage</th>\n",
              "      <th>content</th>\n",
              "      <th>score</th>\n",
              "      <th>thumbsUpCount</th>\n",
              "      <th>reviewCreatedVersion</th>\n",
              "      <th>at</th>\n",
              "      <th>replyContent</th>\n",
              "      <th>repliedAt</th>\n",
              "      <th>sortOrder</th>\n",
              "      <th>appId</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Andrew Thomas</td>\n",
              "      <td>https://lh3.googleusercontent.com/a-/AOh14GiHd...</td>\n",
              "      <td>Update: After getting a response from the deve...</td>\n",
              "      <td>1</td>\n",
              "      <td>21</td>\n",
              "      <td>4.17.0.3</td>\n",
              "      <td>2020-04-05 22:25:57</td>\n",
              "      <td>According to our TOS, and the term you have ag...</td>\n",
              "      <td>2020-04-05 15:10:24</td>\n",
              "      <td>most_relevant</td>\n",
              "      <td>com.anydo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Craig Haines</td>\n",
              "      <td>https://lh3.googleusercontent.com/-hoe0kwSJgPQ...</td>\n",
              "      <td>Used it for a fair amount of time without any ...</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>4.17.0.3</td>\n",
              "      <td>2020-04-04 13:40:01</td>\n",
              "      <td>It sounds like you logged in with a different ...</td>\n",
              "      <td>2020-04-05 15:11:35</td>\n",
              "      <td>most_relevant</td>\n",
              "      <td>com.anydo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>steven adkins</td>\n",
              "      <td>https://lh3.googleusercontent.com/a-/AOh14GiXw...</td>\n",
              "      <td>Your app sucks now!!!!! Used to be good but no...</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>4.17.0.3</td>\n",
              "      <td>2020-04-01 16:18:13</td>\n",
              "      <td>This sounds odd! We are not aware of any issue...</td>\n",
              "      <td>2020-04-02 16:05:56</td>\n",
              "      <td>most_relevant</td>\n",
              "      <td>com.anydo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Lars Panzerbjørn</td>\n",
              "      <td>https://lh3.googleusercontent.com/a-/AOh14Gg-h...</td>\n",
              "      <td>It seems OK, but very basic. Recurring tasks n...</td>\n",
              "      <td>1</td>\n",
              "      <td>192</td>\n",
              "      <td>4.17.0.2</td>\n",
              "      <td>2020-03-12 08:17:34</td>\n",
              "      <td>We do offer this option as part of the Advance...</td>\n",
              "      <td>2020-03-15 06:20:13</td>\n",
              "      <td>most_relevant</td>\n",
              "      <td>com.anydo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Scott Prewitt</td>\n",
              "      <td>https://lh3.googleusercontent.com/-K-X1-YsVd6U...</td>\n",
              "      <td>Absolutely worthless. This app runs a prohibit...</td>\n",
              "      <td>1</td>\n",
              "      <td>42</td>\n",
              "      <td>4.17.0.2</td>\n",
              "      <td>2020-03-14 17:41:01</td>\n",
              "      <td>We're sorry you feel this way! 90% of the app ...</td>\n",
              "      <td>2020-03-15 23:45:51</td>\n",
              "      <td>most_relevant</td>\n",
              "      <td>com.anydo</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6b97c99e-a42f-49fb-aca3-e2cd599508af')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6b97c99e-a42f-49fb-aca3-e2cd599508af button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6b97c99e-a42f-49fb-aca3-e2cd599508af');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ],
      "source": [
        "df = pd.read_csv('drive/MyDrive/sa_dataset/reviews.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEB-hrNsbAgo",
        "outputId": "ce479415-8fb7-4594-bd1f-957d90aea850"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 15746 entries, 0 to 15745\n",
            "Data columns (total 11 columns):\n",
            " #   Column                Non-Null Count  Dtype \n",
            "---  ------                --------------  ----- \n",
            " 0   userName              15746 non-null  object\n",
            " 1   userImage             15746 non-null  object\n",
            " 2   content               15746 non-null  object\n",
            " 3   score                 15746 non-null  int64 \n",
            " 4   thumbsUpCount         15746 non-null  int64 \n",
            " 5   reviewCreatedVersion  13533 non-null  object\n",
            " 6   at                    15746 non-null  object\n",
            " 7   replyContent          7367 non-null   object\n",
            " 8   repliedAt             7367 non-null   object\n",
            " 9   sortOrder             15746 non-null  object\n",
            " 10  appId                 15746 non-null  object\n",
            "dtypes: int64(2), object(9)\n",
            "memory usage: 1.3+ MB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "8fCYclIOdvWr",
        "outputId": "b0a2ebb9-8ebd-42c6-81b3-91e3e92affca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'review scores')"
            ]
          },
          "metadata": {},
          "execution_count": 142
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "sns.countplot(df['score'])\n",
        "plt.xlabel('review scores')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "a0RhAf-DeV7r"
      },
      "outputs": [],
      "source": [
        "class_names = ['negative', 'neutral', 'positive']\n",
        "\n",
        "def to_sentiment(rating):\n",
        "  if rating <= 2:\n",
        "    return 0\n",
        "  elif rating == 3:\n",
        "    return 1\n",
        "  else:\n",
        "    return 2\n",
        "\n",
        "df['sentiment'] = df['score'].apply(to_sentiment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "id": "Bq-Tn83De06j",
        "outputId": "e51632e1-b909-499d-bff0-51721c1c3a18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([<matplotlib.axis.XTick at 0x7f7fa0e621d0>,\n",
              "  <matplotlib.axis.XTick at 0x7f81b024d690>,\n",
              "  <matplotlib.axis.XTick at 0x7f81b024d2d0>],\n",
              " [Text(0, 0, 'negative'), Text(0, 0, 'neutral'), Text(0, 0, 'positive')])"
            ]
          },
          "metadata": {},
          "execution_count": 144
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "sns.countplot(df['sentiment'])\n",
        "plt.xlabel('review sentiment')\n",
        "plt.xticks(range(len(class_names)), class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {
        "id": "ihvy9VA9lSq6"
      },
      "outputs": [],
      "source": [
        "token_lens = []\n",
        "for txt in df['content']:\n",
        "  token_lens.append(len(txt))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "4MQIomXJnP17",
        "outputId": "3ee90bf9-d108-45dd-81fa-48e3a43f9dee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 6.79999999999999, 'Token length')"
            ]
          },
          "metadata": {},
          "execution_count": 199
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "sns.displot(token_lens)\n",
        "plt.xlim([0, 256])\n",
        "plt.xlabel('Token length')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "nyjlvHdIz1F4"
      },
      "outputs": [],
      "source": [
        "class GPReviewDataset(Dataset):\n",
        "\n",
        "  def __init__(self, reviews, targets, tokenizer, max_len):\n",
        "    self.reviews = reviews\n",
        "    self.targets = targets\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.reviews)\n",
        "  \n",
        "  def __getitem__(self, item):\n",
        "    review = str(self.reviews[item])\n",
        "    target = self.targets[item]\n",
        "\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "      review,\n",
        "      add_special_tokens=True,\n",
        "      max_length=self.max_len,\n",
        "      return_token_type_ids=False,\n",
        "      pad_to_max_length=True,\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt',\n",
        "    )\n",
        "\n",
        "    return {\n",
        "      'review_text': review,\n",
        "      'input_ids': encoding['input_ids'].flatten(),\n",
        "      'attention_mask': encoding['attention_mask'].flatten(),\n",
        "      'targets': torch.tensor(target, dtype=torch.long)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "a4nl5Bsv0HmH"
      },
      "outputs": [],
      "source": [
        "df_train, df_test = train_test_split(df, test_size=0.1, random_state=RANDOM_SEED)\n",
        "df_val, df_test = train_test_split(df_test, test_size=0.5, random_state=RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "ifmW8woQ0Nj2"
      },
      "outputs": [],
      "source": [
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "  ds = GPReviewDataset(\n",
        "    reviews=df.content.to_numpy(),\n",
        "    targets=df.sentiment.to_numpy(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=max_len\n",
        "  )\n",
        "\n",
        "  return DataLoader(\n",
        "    ds,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=4\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rs0WAgcy0QUN",
        "outputId": "3dc578a8-ab8f-4711-8c04-d58b670d5a3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 16\n",
        "MAX_LEN = 160\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "iDTyTzj-0eaV"
      },
      "outputs": [],
      "source": [
        "class SentimentClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self, n_classes):\n",
        "    super(SentimentClassifier, self).__init__()\n",
        "    self.bert = BertModel.from_pretrained('bert-base-cased')\n",
        "    self.drop = nn.Dropout(p=0.3)\n",
        "    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "  \n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    _, pooled_output = self.bert(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask,\n",
        "      return_dict=False\n",
        "    )\n",
        "    output = self.drop(pooled_output)\n",
        "    return self.out(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9DpOEYq1-Uo",
        "outputId": "d240bd23-229a-4697-f670-e5350c5783da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "model = SentimentClassifier(len(class_names)).to(device)\n",
        "EPOCHS = 10\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "id": "Nq9AVbiY2UuM"
      },
      "outputs": [],
      "source": [
        "def train_epoch(\n",
        "  model, \n",
        "  data_loader, \n",
        "  loss_fn, \n",
        "  optimizer, \n",
        "  device, \n",
        "  scheduler, \n",
        "  n_examples\n",
        "):\n",
        "  model = model.train()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  \n",
        "  for d in data_loader:\n",
        "    input_ids = d[\"input_ids\"].to(device)\n",
        "    attention_mask = d[\"attention_mask\"].to(device)\n",
        "    targets = d[\"targets\"].to(device)\n",
        "\n",
        "    outputs = model(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    loss = loss_fn(outputs, targets)\n",
        "\n",
        "    correct_predictions += torch.sum(preds == targets)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "      loss = loss_fn(outputs, targets)\n",
        "\n",
        "      correct_predictions += torch.sum(preds == targets)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "metadata": {
        "id": "RVjTw89gSiNG"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0owZVhRO2alc",
        "outputId": "dbd5a811-8676-45e8-aeb0-3ca4cf7595bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.7219670564608972 accuracy 0.6772281419801003\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.5799329876899719 accuracy 0.770012706480305\n",
            "\n",
            "Epoch 2/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.416374773468242 accuracy 0.8404488038952791\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.5735668759793043 accuracy 0.8157560355781449\n",
            "\n",
            "Epoch 3/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.23216352817885216 accuracy 0.9230117846305836\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.6177975972462445 accuracy 0.866581956797967\n",
            "\n",
            "Epoch 4/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.1558911537175523 accuracy 0.9549784771716887\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.740138697961811 accuracy 0.8716645489199493\n",
            "\n",
            "Epoch 5/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.1052684968431495 accuracy 0.9712793733681463\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.834357176477206 accuracy 0.8614993646759848\n",
            "\n",
            "Epoch 6/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.08529858315586832 accuracy 0.9763601721826265\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.849584765410982 accuracy 0.8691232528589581\n",
            "\n",
            "Epoch 7/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.06876080663587715 accuracy 0.9804530378942912\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.7967848177760607 accuracy 0.8818297331639137\n",
            "\n",
            "Epoch 8/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.050894476209327515 accuracy 0.9840519370545481\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.8623135152130271 accuracy 0.8805590851334181\n",
            "\n",
            "Epoch 9/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.04405603689744934 accuracy 0.9865217698115871\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.8602638109930558 accuracy 0.8742058449809403\n",
            "\n",
            "Epoch 10/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.03358710576515151 accuracy 0.988285936066615\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.8530565932458558 accuracy 0.8792884371029225\n",
            "\n",
            "CPU times: user 44min 7s, sys: 20min 21s, total: 1h 4min 28s\n",
            "Wall time: 1h 5min 48s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "history = defaultdict(list)\n",
        "best_accuracy = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "  print('-' * 10)\n",
        "\n",
        "  train_acc, train_loss = train_epoch(\n",
        "    model,\n",
        "    train_data_loader,    \n",
        "    loss_fn, \n",
        "    optimizer, \n",
        "    device, \n",
        "    scheduler, \n",
        "    len(df_train)\n",
        "  )\n",
        "\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "  val_acc, val_loss = eval_model(\n",
        "    model,\n",
        "    val_data_loader,\n",
        "    loss_fn, \n",
        "    device, \n",
        "    len(df_val)\n",
        "  )\n",
        "\n",
        "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "  print()\n",
        "\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "\n",
        "  if val_acc > best_accuracy:\n",
        "    torch.save(model.state_dict(), 'best_model_state.bin')\n",
        "    best_accuracy = val_acc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history['train_acc'] = [history['train_acc'][i].to('cpu') for i in range(len(history['train_acc']))]\n",
        "history['val_acc'] = [history['val_acc'][i].to('cpu') for i in range(len(history['val_acc']))]\n",
        "\n",
        "plt.plot(history['train_acc'], label='train accuracy')\n",
        "plt.plot(history['val_acc'], label='validation accuracy')\n",
        "\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0, 1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "nBelEPiwqnV6",
        "outputId": "317db45d-1700-4afe-953b-48517876617d"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 178
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc, _ = eval_model(model, test_data_loader, loss_fn, device,len(df_test))\n",
        "\n",
        "test_acc.item()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haxIcnTzd8YQ",
        "outputId": "a48e7e55-f8bd-483e-888f-e4e6b19057fa"
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8934010152284263"
            ]
          },
          "metadata": {},
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_predictions(model, data_loader):\n",
        "  model = model.eval()\n",
        "  \n",
        "  review_texts = []\n",
        "  predictions = []\n",
        "  prediction_probs = []\n",
        "  real_values = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "\n",
        "      texts = d[\"review_text\"]\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "      probs = F.softmax(outputs, dim=1)\n",
        "\n",
        "      review_texts.extend(texts)\n",
        "      predictions.extend(preds)\n",
        "      prediction_probs.extend(probs)\n",
        "      real_values.extend(targets)\n",
        "\n",
        "  predictions = torch.stack(predictions).cpu()\n",
        "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
        "  real_values = torch.stack(real_values).cpu()\n",
        "  return review_texts, predictions, prediction_probs, real_values"
      ],
      "metadata": {
        "id": "JEh1weEoeA7R"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(model, test_data_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9AgFz6ceDO3",
        "outputId": "1b468867-feae-4330-e0e2-0b122184a7c4"
      },
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, y_pred, target_names=class_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-Q-dOgkeE3V",
        "outputId": "d97203f0-4500-4694-ac8f-ef8e65c2a41d"
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.92      0.85      0.89       245\n",
            "     neutral       0.82      0.90      0.86       254\n",
            "    positive       0.94      0.92      0.93       289\n",
            "\n",
            "    accuracy                           0.89       788\n",
            "   macro avg       0.89      0.89      0.89       788\n",
            "weighted avg       0.90      0.89      0.89       788\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def show_confusion_matrix(confusion_matrix):\n",
        "  hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
        "  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
        "  plt.ylabel('True sentiment')\n",
        "  plt.xlabel('Predicted sentiment');\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
        "show_confusion_matrix(df_cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "HAEIwazCeGul",
        "outputId": "dd0ef569-cb8d-4877-801f-0d61f58b0e6a"
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 2\n",
        "\n",
        "review_text = y_review_texts[idx]\n",
        "true_sentiment = y_test[idx]\n",
        "pred_df = pd.DataFrame({\n",
        "  'class_names': class_names,\n",
        "  'values': y_pred_probs[idx]\n",
        "})"
      ],
      "metadata": {
        "id": "K8keI4hOeKaN"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pred_df)\n",
        "print()\n",
        "print(f'True sentiment: {class_names[true_sentiment]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhupbwGceM28",
        "outputId": "4c9930b9-fbd8-41a6-cbdc-abb184a372a9"
      },
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  class_names   values\n",
            "0    negative  0.00003\n",
            "1     neutral  0.99994\n",
            "2    positive  0.00003\n",
            "\n",
            "True sentiment: neutral\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.barplot(x='values', y='class_names', data=pred_df, orient='h')\n",
        "plt.ylabel('sentiment')\n",
        "plt.xlabel('probability')\n",
        "plt.xlim([0, 1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "UN268UV1eOq2",
        "outputId": "e456fc65-6d5a-4d45-a4f1-5eb8a7016b90"
      },
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 192
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "review_text = \"It wonderful!!!\"\n",
        "\n",
        "encoded_review = tokenizer.encode_plus(\n",
        "  review_text,\n",
        "  max_length=MAX_LEN,\n",
        "  add_special_tokens=True,\n",
        "  return_token_type_ids=False,\n",
        "  pad_to_max_length=True,\n",
        "  return_attention_mask=True,\n",
        "  return_tensors='pt',\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9WkgsQ0eXS1",
        "outputId": "c019a67a-b0cb-43d9-df4c-7ef8a4a9dbbf"
      },
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = encoded_review['input_ids'].to(device)\n",
        "attention_mask = encoded_review['attention_mask'].to(device)\n",
        "\n",
        "output = model(input_ids, attention_mask)\n",
        "_, prediction = torch.max(output, dim=1)\n",
        "\n",
        "print(f'Review text: {review_text}')\n",
        "print(f'Sentiment  : {class_names[prediction]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWStMVYAeb7H",
        "outputId": "74bda3ba-d057-4f00-b6c9-624817bdf453"
      },
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review text: It wonderful!!!\n",
            "Sentiment  : positive\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "sentiment_analysis.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMZ48bk5XooJeq5uTv2OGR0",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}